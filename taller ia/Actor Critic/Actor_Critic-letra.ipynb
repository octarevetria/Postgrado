{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b40433",
   "metadata": {},
   "source": [
    "# Control con Función de Política: Actor-Critic (TD)\n",
    "\n",
    "En este notebook implementaremos el algoritmo **Actor-Critic con TD (episódico)**, una técnica que combina la estimación de valores y la mejora de política en paralelo. A diferencia de los métodos puramente basados en valores como Q-learning o SARSA, los métodos Actor-Critic mantienen dos funciones diferenciadas: el **actor**, que representa la política, y el **crítico**, que estima el valor de los estados (o pares estado-acción).\n",
    "\n",
    "Este enfoque aprovecha la estabilidad de las actualizaciones del valor para guiar el aprendizaje de la política directamente, haciendo uso del gradiente de política y del error de TD. Esto permite manejar entornos con espacios de acción continuos o políticas estocásticas.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "* Implementar el algoritmo **Actor-Critic (episódico)** utilizando PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95148d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61db27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.name == 'posix' and os.uname().sysname == 'Darwin':\n",
    "    # Set the path to ffmpeg for macOS, replace with your actual path\n",
    "    os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc83638",
   "metadata": {},
   "source": [
    "## Definir constantes y funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9a896c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cpu'  # por defecto, usamos la CPU\n",
    "if torch.cuda.is_available():  # si hay una GPU disponible (y cuda está instalado)\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.backends.mps.is_available():  # si hay un MPS disponible (Metal Performance Shaders)\n",
    "    DEVICE = 'mps'\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a458e2b",
   "metadata": {},
   "source": [
    "Podemos cambiar el ambiente para probar otros casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "431c765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVS = [\"MountainCar-v0\", \"CartPole-v1\"]\n",
    "ENV_NAME = ENVS[1]  # Cambia esto para probar con otros entornos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ba5522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(env_name, record_video=False, record_every=1, folder=\"./videos\" ):\n",
    "    \"\"\"\n",
    "    Create the environment with optional video recording and statistics.\n",
    "    Args:\n",
    "        env_name (str): Name of the environment to create.\n",
    "        record_video (bool): Whether to record video of the episodes.\n",
    "        record_every (int): Frequency of recording episodes.\n",
    "        folder (str): Folder to save the recorded videos.\n",
    "    Returns:\n",
    "        env (gym.Env): The environment.\n",
    "        \n",
    "    See also:\n",
    "        https://gymnasium.farama.org/introduction/record_agent/\n",
    "    \"\"\"\n",
    "    # Initialise the environment\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "    if record_video:\n",
    "        env = RecordVideo(env, video_folder=folder,\n",
    "                    episode_trigger=lambda x: x % record_every == 0)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144d920a",
   "metadata": {},
   "source": [
    "Exploramos minimamente el entorno (Observation space y Action space) para entender cómo interactuar con él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ee6e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: CartPole-v1\n",
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Observation space shape: (4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = get_env(ENV_NAME)\n",
    "print(f\"Environment: {ENV_NAME}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Observation space shape: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "INPUT_DIM = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdf79d",
   "metadata": {},
   "source": [
    "Definimos una función que nos pase nuestra observación a un tensor de PyTorch, lo cual es necesario para trabajar con redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f13a021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state(obs, device=DEVICE):\n",
    "    return torch.tensor(obs, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdb5b6",
   "metadata": {},
   "source": [
    "## Redes (función de política y función de valor)\n",
    "\n",
    "Vamos a definir dos redes neuronales: una para la política (actor) y otra para el valor (crítico). Ambas redes tendrán una arquitectura simple. Lo importante es que la red de política saldrá una distribución de probabilidad sobre las acciones, mientras que la red de valor saldrá un valor escalar para el estado actual.\n",
    "\n",
    "> La red de política (actor) se encargará de seleccionar acciones basadas en la política aprendida, mientras que la red de valor (crítico) evaluará el estado actual y proporcionará retroalimentación al actor. Es importate e ultizar una función de activación adecuada para la salida de la red de política, como `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2b046ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1185, 0.0059, 0.8756]], device='cuda:0')\n",
      "0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "tensor_actor_test = torch.tensor([1.0, -2.0, 3.0], device=DEVICE).unsqueeze(0)  # Añadimos una dimensión para simular un batch\n",
    "print(f\"{torch.nn.Softmax(dim=-1)(tensor_actor_test)}\") \n",
    "print(f\"{torch.nn.Softmax(dim=-1)(tensor_actor_test).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75d36e",
   "metadata": {},
   "source": [
    "### Definir la red de política (actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93e05efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ActorCNN                                 [1, 2]                    --\n",
       "├─Linear: 1-1                            [1, 128]                  640\n",
       "├─Linear: 1-2                            [1, 2]                    258\n",
       "==========================================================================================\n",
       "Total params: 898\n",
       "Trainable params: 898\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActorCNN(nn.Module):\n",
    "    def __init__(self, input_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, n_actions)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "actor_model = ActorCNN(INPUT_DIM, N_ACTIONS).to(DEVICE)\n",
    "\n",
    "summary(actor_model, input_size=(1, INPUT_DIM), device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f298d",
   "metadata": {},
   "source": [
    "Probamos con un tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a68b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0805, 0.0557]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset() # obtenermos un estado inicial del entorno\n",
    "tensor_obs_test = process_state(obs, device=DEVICE) # lo pasamos a un tensor\n",
    "actor_model(tensor_obs_test) # lo pasamos por la red neuronal (debería devolver una distribución de probabilidad sobre las acciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b8895",
   "metadata": {},
   "source": [
    "### Definir la red valor (crítico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c02d8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CriticCNN                                [1, 1]                    --\n",
       "├─Linear: 1-1                            [1, 128]                  640\n",
       "├─Linear: 1-2                            [1, 1]                    129\n",
       "==========================================================================================\n",
       "Total params: 769\n",
       "Trainable params: 769\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CriticCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "critic_model = CriticCNN(INPUT_DIM).to(DEVICE)\n",
    "\n",
    "summary(critic_model, input_size=(1, INPUT_DIM), device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd86cbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0047]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset() # obtenermos un estado inicial del entorno\n",
    "tensor_obs_test = process_state(obs, device=DEVICE) # lo pasamos a un tensor\n",
    "critic_model(tensor_obs_test) # lo pasamos por la red neuronal (esto debería devolver un valor de estado, no una distribución de probabilidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8eac8f",
   "metadata": {},
   "source": [
    "## Sampleo de acciones\n",
    "Para samplear acciones vamos a utilizar la red de política. Dado que la salida de la red de política es una distribución de probabilidad, utilizaremos `torch.distributions.Categorical` para muestrear acciones basadas en esta distribución.\n",
    "\n",
    "El paquete `torch.distributions` nos permite trabajar con distribuciones probabilísticas de manera sencilla, pudienendo muestrear acciones y calcular log-probabilidades de las acciones seleccionadas.\n",
    "\n",
    "Dichos tensores (log-probabilidades) contienen los gradientes necesarios para actualizar la red de política durante el entrenamiento.\n",
    "\n",
    "Ver [torch.distributions](https://docs.pytorch.org/docs/stable/distributions.html) y [Categorical](https://docs.pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "294eef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades: tensor([[0.1000, 0.2000, 0.7000]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "Acción muestreada: tensor([2], device='cuda:0')\n",
      "Log-probabilidades: tensor([-0.3567], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "prob_tensor = torch.tensor([0.1, 0.2, 0.7], device=DEVICE, requires_grad=True).unsqueeze(0)  # Simulamos una distribución de probabilidad\n",
    "distribtion = torch.distributions.Categorical(prob_tensor)\n",
    "print(f\"Probabilidades: {prob_tensor}\")\n",
    "action = distribtion.sample()  # Muestreamos una acción de la distribución\n",
    "print(f\"Acción muestreada: {action}\")\n",
    "print(f\"Log-probabilidades: {distribtion.log_prob(action)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e70dc2",
   "metadata": {},
   "source": [
    "## Algoritmo Actor-Critic\n",
    "\n",
    "**Input:**  \n",
    "- una parametrización diferenciable de la política $\\pi(a|s, \\theta)$  \n",
    "- una parametrización diferenciable de la función de valor de estado $\\hat{v}(s, \\mathbf{w})$  \n",
    "\n",
    "**Parámetros:** tasas de aprendizaje $\\alpha^\\theta > 0$, $\\alpha^{\\mathbf{w}} > 0$\n",
    "\n",
    "**Inicializar:**  \n",
    "- parámetros de la política $\\theta \\in \\mathbb{R}^{d'}$\n",
    "- pesos del valor de estado $\\mathbf{w} \\in \\mathbb{R}^d$ (por ejemplo, a $0$)\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Loop forever (para cada episodio):} \\\\\n",
    "\\quad \\text{Inicializar } S \\text{ (primer estado del episodio)} \\\\\n",
    "\\quad I \\leftarrow 1 \\\\\n",
    "\\quad \\textbf{Loop mientras } S \\text{ no sea terminal (para cada paso de tiempo):} \\\\\n",
    "\\quad\\quad A \\sim \\pi(\\cdot|S, \\theta) \\\\\n",
    "\\quad\\quad \\text{Tomar acción } A, \\text{ observar } S', R \\\\\n",
    "\\quad\\quad \\delta \\leftarrow R + \\gamma \\hat{v}(S', \\mathbf{w}) - \\hat{v}(S, \\mathbf{w}) \\quad (\\text{si } S' \\text{ es terminal, } \\hat{v}(S', \\mathbf{w}) \\doteq 0) \\\\\n",
    "\\quad\\quad \\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha^{\\mathbf{w}} \\delta \\nabla \\hat{v}(S, \\mathbf{w}) \\\\\n",
    "\\quad\\quad \\theta \\leftarrow \\theta + \\alpha^{\\theta} I \\delta \\nabla \\ln \\pi(A|S, \\theta) \\\\\n",
    "\\quad\\quad I \\leftarrow \\gamma I \\\\\n",
    "\\quad\\quad S \\leftarrow S' \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_probs):\n",
    "    distribtion = torch.distributions.Categorical(action_probs)\n",
    "    action = distribtion.sample()\n",
    "    log_prob = distribtion.log_prob(action)\n",
    "    return action.item(), log_prob\n",
    "\n",
    "\n",
    "def train(env, actor_net, critic_net, process_state_fn, num_episodes=10_000, actor_lr=.0001, critic_lr=.0001, gamma=.99):\n",
    "    actor_opt = torch.optim.Adam(actor_net.parameters(), lr=actor_lr)\n",
    "    critic_opt = torch.optim.Adam(critic_net.parameters(), lr = critic_lr)\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        obs, _ = env.reset()\n",
    "        obs_t = process_state_fn(obs)\n",
    "        I = 1\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob = select_action(actor_net(obs_t))\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_obs_t = process_state(next_obs)\n",
    "\n",
    "            obs_value = critic_net(obs_t)\n",
    "            next_obs_value = critic_net(next_obs_t) * (1-float(done))\n",
    "\n",
    "            delta = reward + (gamma * next_obs_value.detach()) - obs_value\n",
    "\n",
    "            #Actualizar los pesos del critic (w)\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss = delta.pow(2)\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "            #Actualizar los pesos del actor (teta)\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss = -I * delta.detach() * log_prob\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "\n",
    "            I = I * gamma\n",
    "            obs = next_obs\n",
    "            obs_t = next_obs_t\n",
    "\n",
    "\n",
    "def play_episodes(env, actor_net, process_state_fn, num_episodes=5):\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            obs_t = process_state_fn(obs)\n",
    "            action, _ = select_action(actor_net(obs_t))\n",
    "            next_obs, _, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_obs_t = process_state(next_obs)\n",
    "\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "628e498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = ActorCNN(INPUT_DIM, N_ACTIONS).to(DEVICE)\n",
    "critic_net = CriticCNN(INPUT_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93d3dd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\Octav\\postgrado\\taller ia\\Actor Critic\\videos\\train folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]<frozen importlib._bootstrap>:241: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "  0%|          | 0/2000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 2)) of distribution Categorical(probs: torch.Size([1, 2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[-1.9793,  2.9793]], device='cuda:0', grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m env = get_env(ENV_NAME, record_video=\u001b[38;5;28;01mTrue\u001b[39;00m, record_every=\u001b[32m100\u001b[39m, folder=\u001b[33m\"\u001b[39m\u001b[33m./videos/train\u001b[39m\u001b[33m\"\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_state_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocess_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0005\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env, actor_net, critic_net, process_state_fn, num_episodes, actor_lr, critic_lr, gamma)\u001b[39m\n\u001b[32m     16\u001b[39m done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     action, log_prob = \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     next_obs, reward, terminated, truncated, _ = env.step(action)\n\u001b[32m     20\u001b[39m     done = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mselect_action\u001b[39m\u001b[34m(action_probs)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect_action\u001b[39m(action_probs):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     distribtion = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     action = distribtion.sample()\n\u001b[32m      4\u001b[39m     log_prob = distribtion.log_prob(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\torch\\distributions\\categorical.py:72\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     69\u001b[39m batch_shape = (\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\torch\\distributions\\distribution.py:71\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter probs (Tensor of shape (1, 2)) of distribution Categorical(probs: torch.Size([1, 2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[-1.9793,  2.9793]], device='cuda:0', grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "env = get_env(ENV_NAME, record_video=True, record_every=100, folder=\"./videos/train\" )\n",
    "train(env, actor_net, critic_net, process_state_fn=process_state, num_episodes=2_000, actor_lr=0.0001, critic_lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Octav\\anaconda3\\envs\\obl_taller_ia\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\Octav\\postgrado\\taller ia\\Actor Critic\\videos\\test folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = get_env(ENV_NAME, record_video=True, record_every=1, folder=\"./videos/test\" )\n",
    "play_episodes(env, actor_net, process_state_fn=process_state, num_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obl_taller_ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
