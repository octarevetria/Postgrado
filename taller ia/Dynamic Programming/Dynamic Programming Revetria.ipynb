{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Octavio Revetria 232745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tarea 2 - Programación Dinámica (Grid World)\n",
    "\n",
    "## Objetivos\n",
    "- Entender y aplicar los conceptos de Programación Dinámica en Aprendizaje por Refuerzo.\n",
    "- Implementar los algoritmos de Evaluación de Política (Policy Evaluation), Mejora de Política (Policy Improvement) e Iteración de Valor (Value Iteration).\n",
    "- Obtener y analizar políticas óptimas en un ambiente de Grid World.\n",
    "\n",
    "## A entregar\n",
    "- Implementación del método de Iteración de Política.\n",
    "- Implementación del método de Iteración de Valor.\n",
    "- Comparación de los resultados obtenidos y discusión sobre convergencia y estabilidad.\n",
    "\n",
    "## Descripción del ambiente a usar\n",
    "\n",
    "Trabajaremos con el ambiente **Grid World** descrito en el capítulo 4 del libro de Sutton y Barto. Se trata de un entorno en forma de cuadrícula donde el agente puede moverse en cuatro direcciones:\n",
    "- **RIGHT = 0** (derecha)\n",
    "- **UP = 1** (arriba)\n",
    "- **LEFT = 2** (izquierda)\n",
    "- **DOWN = 3** (abajo)\n",
    "\n",
    "El objetivo del agente es alcanzar uno de los dos estados terminales ubicados en la esquina superior izquierda y la esquina inferior derecha de la cuadrícula. Cada movimiento tiene un costo de -1, incentivando así la búsqueda de la ruta más corta hacia un estado terminal. Si el agente intenta moverse fuera de los límites de la cuadrícula, permanecerá en su posición actual.\n",
    "\n",
    "Un ejemplo de un Grid World 4x4:\n",
    "```\n",
    "T  o  o  o\n",
    "o  x  o  o\n",
    "o  o  o  o\n",
    "o  o  o  T\n",
    "```\n",
    "- \"T\" representa un estado terminal.\n",
    "- \"x\" representa la posición actual del agente.\n",
    "- \"o\" representa los estados transitables.\n",
    "\n",
    "La dinámica de la recompensa es simple: el agente recibe un **recompensa de -1** en cada paso hasta alcanzar un estado terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quizás se necesita:\n",
    "> https://anaconda.org/conda-forge/gymnasium-box2d\n",
    "> https://anaconda.org/conda-forge/gymnasium-other\n",
    "> https://ffmpeg.org/download.html (en mac: `brew install ffmpeg`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import numpy as np\n",
    "import GridWorldEnv\n",
    "\n",
    "from Utils import print_state_values, print_state_action_values, print_policy_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas constantes\n",
    "\n",
    "HUMAN_RENDER = False # False si queremos guardar el episodio en un video, True si queremos verlo en tiempo real (no disponible en Google Colab u otros entornos sin interfaz gráfica)\n",
    "\n",
    "RECORD_VIDEO = True # True si queremos guardar el episodio en un video, quizas se necesita: https://anaconda.org/conda-forge/gymnasium-other\n",
    "RECORD_EVERY = 1 # Cada cuántos episodios guardamos un video\n",
    "\n",
    "GRID_SIZE = 4 # Tamaño del grid\n",
    "GAMMA = 1 # Factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Wrapper\n",
    "\n",
    "En Gymnasium, los [wrappers](https://gymnasium.farama.org/api/wrappers/) son herramientas que permiten modificar el comportamiento de los entornos sin alterar su código base. Estos *wrappers* se utilizan para extender funcionalidades, como la normalización de observaciones, limitación de acciones o registro de estadísticas y videos durante el entrenamiento de agentes.\n",
    "\n",
    "Para este ejercicio, se utilizarán dos *wrappers* de Gymnasium:\n",
    "\n",
    "**`RecordVideo`**: Este *wrapper* permite grabar videos de las ejecuciones del agente en el entorno. Es especialmente útil para visualizar el comportamiento del agente y evaluar su desempeño. Se puede configurar para grabar todos los episodios o solo episodios específicos, según una función de activación definida por el usuario.\n",
    "\n",
    "Documentación oficial de Gymnasium sobre [grabación de agentes](https://gymnasium.farama.org/introduction/record_agent/) y la [API de wrappers](https://gymnasium.farama.org/api/wrappers/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_environment(human_render=HUMAN_RENDER, record_video=RECORD_VIDEO, record_every=RECORD_EVERY, grid_size=GRID_SIZE):\n",
    "    assert not (human_render and record_video), \"No se puede renderizar en tiempo real y guardar un video a la vez\"\n",
    "    \n",
    "    render_mode = \"human\" if human_render else \"rgb_array\"\n",
    "    \n",
    "    # Initialise the environment\n",
    "    env = gym.make(\"gymnasium_env/GridWorld-v0\", render_mode=render_mode, size=grid_size)\n",
    "\n",
    "    if record_video:\n",
    "        env = RecordVideo(env, video_folder=\"./videos\", name_prefix=\"gridworld\",\n",
    "                    episode_trigger=lambda x: x % record_every == 0)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el entorno y sus wrappers con un agente aleatorio para visualizar el comportamiento del entorno y la grabación de videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"C:/Users/Octav/anaconda3/envs/ia-taller/Library/bin/ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "# We play 10 episodes, each one with a random policy\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        action = env.action_space.sample()  # random policy\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación de la Política $\\pi$\n",
    "\n",
    "Para implementar los algoritmos de programación dinámica, necesitamos definir cómo representaremos la política $\\pi(s, a)$. \n",
    "\n",
    "### Estructura de la Política\n",
    "La política se almacenará en un **diccionario** de la forma:\n",
    "\n",
    "```python\n",
    "pi = {\n",
    "    ((x, y), a): probabilidad\n",
    "}\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- $(x, y)$ representa el estado en la grilla.\n",
    "- $a$ representa una de las cuatro acciones posibles.\n",
    "- `probabilidad` es la probabilidad de tomar la acción $a$ en el estado $(x, y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función auxiliar para saber si estamos en un estado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(state):\n",
    "    return state in [(0, 0), (GRID_SIZE - 1, GRID_SIZE - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Uniforme\n",
    "Una política uniforme en la que todas las acciones tienen la misma probabilidad ($ 25\\% $) se define como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 1), 0): 0.25, ((0, 1), 1): 0.25, ((0, 1), 2): 0.25, ((0, 1), 3): 0.25, ((0, 2), 0): 0.25, ((0, 2), 1): 0.25, ((0, 2), 2): 0.25, ((0, 2), 3): 0.25, ((0, 3), 0): 0.25, ((0, 3), 1): 0.25, ((0, 3), 2): 0.25, ((0, 3), 3): 0.25, ((1, 0), 0): 0.25, ((1, 0), 1): 0.25, ((1, 0), 2): 0.25, ((1, 0), 3): 0.25, ((1, 1), 0): 0.25, ((1, 1), 1): 0.25, ((1, 1), 2): 0.25, ((1, 1), 3): 0.25, ((1, 2), 0): 0.25, ((1, 2), 1): 0.25, ((1, 2), 2): 0.25, ((1, 2), 3): 0.25, ((1, 3), 0): 0.25, ((1, 3), 1): 0.25, ((1, 3), 2): 0.25, ((1, 3), 3): 0.25, ((2, 0), 0): 0.25, ((2, 0), 1): 0.25, ((2, 0), 2): 0.25, ((2, 0), 3): 0.25, ((2, 1), 0): 0.25, ((2, 1), 1): 0.25, ((2, 1), 2): 0.25, ((2, 1), 3): 0.25, ((2, 2), 0): 0.25, ((2, 2), 1): 0.25, ((2, 2), 2): 0.25, ((2, 2), 3): 0.25, ((2, 3), 0): 0.25, ((2, 3), 1): 0.25, ((2, 3), 2): 0.25, ((2, 3), 3): 0.25, ((3, 0), 0): 0.25, ((3, 0), 1): 0.25, ((3, 0), 2): 0.25, ((3, 0), 3): 0.25, ((3, 1), 0): 0.25, ((3, 1), 1): 0.25, ((3, 1), 2): 0.25, ((3, 1), 3): 0.25, ((3, 2), 0): 0.25, ((3, 2), 1): 0.25, ((3, 2), 2): 0.25, ((3, 2), 3): 0.25}\n"
     ]
    }
   ],
   "source": [
    "pi_rand = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        for a in range(4):  # Cuatro acciones posibles\n",
    "            if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "                continue\n",
    "            pi_rand[((x, y), a)] = 1 / 4  # Probabilidad uniforme\n",
    "\n",
    "print(pi_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Determinista (derecha y abajo)\n",
    "Si queremos que el agente siempre elija moverse a la derecha (acción `0`) a excepción de la ultima columna que se mueve hacia abajo (accion `3`), podemos definir la política de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 1), 0): 1.0, ((0, 2), 0): 1.0, ((0, 3), 0): 1.0, ((1, 0), 0): 1.0, ((1, 1), 0): 1.0, ((1, 2), 0): 1.0, ((1, 3), 0): 1.0, ((2, 0), 0): 1.0, ((2, 1), 0): 1.0, ((2, 2), 0): 1.0, ((2, 3), 0): 1.0, ((3, 0), 3): 1, ((3, 1), 3): 1, ((3, 2), 3): 1}\n"
     ]
    }
   ],
   "source": [
    "pi_der_aba = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "            continue\n",
    "        if x == GRID_SIZE - 1:  # Si estamos en la última columna, solo podemos movernos hacia abajo\n",
    "            pi_der_aba[((x, y), 3)] = 1\n",
    "        else:  # En otro caso, podemos movernos hacia la derecha\n",
    "            pi_der_aba[((x, y), 0)] = 1.0\n",
    "print(pi_der_aba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Política Determinista (siempre abajo)\n",
    "Si queremos que el agente siempre elija moverse hacia abajo (acción `3`), podemos definir la política de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_abajo = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "                continue\n",
    "        pi_abajo[((x, y), 3)] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pi_abajo.get(((2,2), a), 0) for a in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25, 0.25, 0.25, 0.25]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pi_rand.get(((2,2), a), 0) for a in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elección de una acción dado un estado y una política\n",
    "Para elegir una acción dado un estado y una política, necesitamos muestrear una acción de acuerdo a la distribución de probabilidad definida en la política. Para esto, podemos utilizar la función `np.random.choice` de NumPy, que nos permite muestrear un elemento de un conjunto dado con una probabilidad dada.\n",
    "\n",
    "Más información sobre `np.random.choice` en la [documentación oficial de NumPy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy):\n",
    "    \"\"\"\n",
    "    Dado un estado y una política, devuelve la acción a tomar.\n",
    "\n",
    "    Parámetros:\n",
    "    - state: Tupla (x, y) representando el estado actual.\n",
    "    - policy: Diccionario {((x, y), a): probabilidad} con la política.\n",
    "\n",
    "    Retorna:\n",
    "    - La acción seleccionada según la política.\n",
    "    \n",
    "    Nota: Si la política es estocástica, se selecciona una acción al azar según las probabilidades.\n",
    "    recomienda npn random choice\n",
    "    \"\"\"\n",
    "    actions = range(4)\n",
    "    prob = [policy.get((state, a), 0) for a in actions]\n",
    "    return np.random.choice(actions, p=prob).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinámica del Ambiente ($ p $) \n",
    "\n",
    "**La dinámica del ambiente $ p(s' | s, a) $**, que define las transiciones entre estados al tomar una acción. Esta información nos permite conocer, para cada estado y acción, cuál será la probabilidad de llegar al próximo estado y la recompensa recibida.\n",
    "\n",
    "> Cuando creamos un ambiente con `gym.make()`, Gymnasium aplica un envoltorio (`OrderEnforcing`) que restringe el acceso directo a ciertos atributos internos. Para acceder a `p`, necesitamos usar `env.unwrapped`, que nos da acceso al objeto subyacente sin restricciones.\n",
    "\n",
    "Ejemplo de acceso a la política desde el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 0), 0): [(1.0, (0, 0), 0.0)], ((0, 0), 1): [(1.0, (0, 0), 0.0)], ((0, 0), 2): [(1.0, (0, 0), 0.0)], ((0, 0), 3): [(1.0, (0, 0), 0.0)], ((0, 1), 0): [(1.0, (1, 1), -1.0)], ((0, 1), 1): [(1.0, (0, 0), -1.0)], ((0, 1), 2): [(1.0, (0, 1), -1.0)], ((0, 1), 3): [(1.0, (0, 2), -1.0)], ((0, 2), 0): [(1.0, (1, 2), -1.0)], ((0, 2), 1): [(1.0, (0, 1), -1.0)], ((0, 2), 2): [(1.0, (0, 2), -1.0)], ((0, 2), 3): [(1.0, (0, 3), -1.0)], ((0, 3), 0): [(1.0, (1, 3), -1.0)], ((0, 3), 1): [(1.0, (0, 2), -1.0)], ((0, 3), 2): [(1.0, (0, 3), -1.0)], ((0, 3), 3): [(1.0, (0, 3), -1.0)], ((1, 0), 0): [(1.0, (2, 0), -1.0)], ((1, 0), 1): [(1.0, (1, 0), -1.0)], ((1, 0), 2): [(1.0, (0, 0), -1.0)], ((1, 0), 3): [(1.0, (1, 1), -1.0)], ((1, 1), 0): [(1.0, (2, 1), -1.0)], ((1, 1), 1): [(1.0, (1, 0), -1.0)], ((1, 1), 2): [(1.0, (0, 1), -1.0)], ((1, 1), 3): [(1.0, (1, 2), -1.0)], ((1, 2), 0): [(1.0, (2, 2), -1.0)], ((1, 2), 1): [(1.0, (1, 1), -1.0)], ((1, 2), 2): [(1.0, (0, 2), -1.0)], ((1, 2), 3): [(1.0, (1, 3), -1.0)], ((1, 3), 0): [(1.0, (2, 3), -1.0)], ((1, 3), 1): [(1.0, (1, 2), -1.0)], ((1, 3), 2): [(1.0, (0, 3), -1.0)], ((1, 3), 3): [(1.0, (1, 3), -1.0)], ((2, 0), 0): [(1.0, (3, 0), -1.0)], ((2, 0), 1): [(1.0, (2, 0), -1.0)], ((2, 0), 2): [(1.0, (1, 0), -1.0)], ((2, 0), 3): [(1.0, (2, 1), -1.0)], ((2, 1), 0): [(1.0, (3, 1), -1.0)], ((2, 1), 1): [(1.0, (2, 0), -1.0)], ((2, 1), 2): [(1.0, (1, 1), -1.0)], ((2, 1), 3): [(1.0, (2, 2), -1.0)], ((2, 2), 0): [(1.0, (3, 2), -1.0)], ((2, 2), 1): [(1.0, (2, 1), -1.0)], ((2, 2), 2): [(1.0, (1, 2), -1.0)], ((2, 2), 3): [(1.0, (2, 3), -1.0)], ((2, 3), 0): [(1.0, (3, 3), -1.0)], ((2, 3), 1): [(1.0, (2, 2), -1.0)], ((2, 3), 2): [(1.0, (1, 3), -1.0)], ((2, 3), 3): [(1.0, (2, 3), -1.0)], ((3, 0), 0): [(1.0, (3, 0), -1.0)], ((3, 0), 1): [(1.0, (3, 0), -1.0)], ((3, 0), 2): [(1.0, (2, 0), -1.0)], ((3, 0), 3): [(1.0, (3, 1), -1.0)], ((3, 1), 0): [(1.0, (3, 1), -1.0)], ((3, 1), 1): [(1.0, (3, 0), -1.0)], ((3, 1), 2): [(1.0, (2, 1), -1.0)], ((3, 1), 3): [(1.0, (3, 2), -1.0)], ((3, 2), 0): [(1.0, (3, 2), -1.0)], ((3, 2), 1): [(1.0, (3, 1), -1.0)], ((3, 2), 2): [(1.0, (2, 2), -1.0)], ((3, 2), 3): [(1.0, (3, 3), -1.0)], ((3, 3), 0): [(1.0, (3, 3), 0.0)], ((3, 3), 1): [(1.0, (3, 3), 0.0)], ((3, 3), 2): [(1.0, (3, 3), 0.0)], ((3, 3), 3): [(1.0, (3, 3), 0.0)]}\n"
     ]
    }
   ],
   "source": [
    "p = env.unwrapped.p\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El diccionario tiene la siguiente forma:\n",
    "\n",
    "```python\n",
    "self.p = {\n",
    "    ((x, y), a): [(probabilidad, (nuevo_x, nuevo_y), recompensa)]\n",
    "}\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `((x, y), a)`: Clave que representa el **estado actual** `(x, y)` y la **acción** `a` tomada.\n",
    "- **Valor asociado**: Una lista con **una o más transiciones posibles** en el formato:\n",
    "  - `probabilidad`: La probabilidad de que ocurra la transición (en este caso, siempre `1.0`, porque el ambiente es determinista).\n",
    "  - `(nuevo_x, nuevo_y)`: El nuevo estado después de tomar la acción.\n",
    "  - `recompensa`: El valor de recompensa por realizar la acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Característica de la Política en este Caso**\n",
    "- **Es determinista**, lo que significa que en cada estado hay una acción con probabilidad 1.\n",
    "- **En los estados terminales, no importa la acción que se tome, simpre se permanece en el mismo estado.**\n",
    "\n",
    "> ¿ Cómo podemos ver en el código estas propiedades ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "La **Iteración de Política** (*Policy Iteration*) es un método fundamental en **Programación Dinámica** para encontrar la política óptima $\\pi^*$ en un proceso de decisión de Markov (*MDP*) finito. Se basa en dos pasos clave que se repiten iterativamente hasta la convergencia:\n",
    "\n",
    "1. **Evaluación de Política** (*Policy Evaluation*): Se calcula el valor de la política actual $\\pi$, es decir, se obtiene la función de valor $V_\\pi(s)$ resolviendo la ecuación de Bellman para todos los estados:\n",
    "   $$\n",
    "   V_\\pi(s) = \\sum_{a} \\pi(a | s) \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V_\\pi(s') \\right]\n",
    "   $$\n",
    "   donde $p(s', r | s, a)$ representa la dinámica del ambiente y $\\gamma$ es el factor de descuento.\n",
    "\n",
    "2. **Mejora de Política** (*Policy Improvement*): Se actualiza la política eligiendo en cada estado la acción que maximiza el valor esperado según la función de acción-valor $Q_\\pi(s, a)$:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_{a} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\gamma V_\\pi(s') \\right]\n",
    "   $$\n",
    "   Si la nueva política $\\pi'$ es diferente de la anterior, se repite el proceso con la nueva política. Si no cambia, hemos encontrado la política óptima $\\pi^*$.\n",
    "\n",
    "Proceso Iterativo:\n",
    "1. Inicializar una política arbitraria $\\pi$.\n",
    "2. Aplicar **Policy Evaluation** para calcular $V_\\pi$.\n",
    "3. Aplicar **Policy Improvement** para obtener una nueva política $\\pi'$.\n",
    "4. Si $\\pi' = \\pi$, detenerse. De lo contrario, repetir desde el paso 2.\n",
    "\n",
    "Este algoritmo garantiza la convergencia a la política óptima en un número finito de iteraciones en ambientes con estados y acciones finitas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Policy evaluation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Input:} \\quad & \\pi,\\ \\text{la política a evaluar.} \\\\\n",
    "\\textbf{Parámetro:} \\quad & \\theta > 0,\\ \\text{umbral pequeño que determina la precisión.} \\\\\n",
    "\\textbf{Inicializar:} \\quad & V(s) \\text{ arbitrariamente para todo } s \\in S^+, \\text{ excepto } V(\\text{terminal}) = 0. \\\\\n",
    "\\\\\n",
    "\\textbf{Repetir:} \\quad & \\Delta \\gets 0 \\\\\n",
    "& \\text{Para cada } s \\in S: \\\\\n",
    "& \\quad\\quad v \\gets V(s) \\\\\n",
    "& \\quad\\quad V(s) \\gets \\sum_{a} \\pi(a \\mid s) \\sum_{s',r} p(s',r \\mid s,a)\\,\\Bigl[r + \\gamma V(s')\\Bigr] \\\\\n",
    "& \\quad\\quad \\Delta \\gets \\max\\Bigl(\\Delta,\\, \\bigl|v - V(s)\\bigr|\\Bigr) \\\\\n",
    "\\\\\n",
    "\\textbf{Hasta que:} \\quad & \\Delta < \\theta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(p, pi, gamma=GAMMA, theta=1e-5, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Evalúa la política 'pi' en el entorno 'env' usando el método de evaluación iterativa.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        pi: politica a evaluar, diccionario que mapea (estado, acción) a su probabilidad en la política.\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        V: función de valor de la política 'pi', diccionario que mapea estado (x, y) a su valor.\n",
    "    \"\"\"\n",
    "    V = {}\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            V[s] = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for y in range(size):\n",
    "            for x in range(size):\n",
    "                s = (x,y)\n",
    "                v = V[s]\n",
    "                V[s] = sum([pi.get((s,a), 0) * prob * (r + gamma * V[s_next]) \n",
    "                            for a in range(4) \n",
    "                            for (prob, s_next, r) in p[(s,a)]])\n",
    "                delta = max(delta, abs (v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00   -14.00   -20.00   -22.00\n",
      "  -14.00   -18.00   -20.00   -20.00\n",
      "  -20.00   -20.00   -18.00   -14.00\n",
      "  -22.00   -20.00   -14.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_pi_random = policy_evaluation(p, pi_rand)\n",
    "print_state_values(V_pi_random, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -5.00    -4.00    -3.00\n",
      "   -5.00    -4.00    -3.00    -2.00\n",
      "   -4.00    -3.00    -2.00    -1.00\n",
      "   -3.00    -2.00    -1.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_da_pi = policy_evaluation(p, pi_der_aba)\n",
    "print_state_values(V_da_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00  -100.00  -100.00    -2.97\n",
      " -100.00  -100.00  -100.00    -1.99\n",
      " -100.00  -100.00  -100.00    -1.00\n",
      " -100.00  -100.00  -100.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_d_pi = policy_evaluation(p, pi_abajo, gamma=0.99)\n",
    "print_state_values(V_d_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de Policy Improvement\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textbf{Input:} \\quad & V,\\ \\text{la función de valor evaluada}, \\\\\n",
    "& p,\\ \\text{la dinamica del ambiente } \\\\\n",
    "& \\gamma,\\ \\text{el factor de descuento.} \\\\\n",
    "\\\\\n",
    "\\textbf{Inicializar:} \\quad & \\text{Para cada } s \\in S \\text{ y } a \\in A(s),\\ new\\_pi(s,a) \\text{ se asigna arbitrariamente.} \\\\\n",
    "\\\\\n",
    "\\textbf{Para cada } s \\in S \\text{ (excepto estados terminales):} \\quad & \\\\\n",
    "& \\quad \\text{Para cada acción } a \\in A(s): \\\\\n",
    "& \\quad\\quad Q(s,a) \\gets \\sum_{s',r} p(s',r \\mid s,a) \\Bigl[ r + \\gamma\\, V(s') \\Bigr] \\\\\n",
    "& \\quad \\text{Definir } best\\_actions = \\{ a \\in A(s) \\mid Q(s,a) = \\max_{a' \\in A(s)} Q(s,a') \\} \\\\\n",
    "& \\quad \\text{Para cada } a \\in A(s): \\\\\n",
    "& \\quad\\quad new\\_pi(s,a) \\gets \n",
    "\\begin{cases}\n",
    "\\displaystyle \\frac{1}{\\lvert best\\_actions \\rvert}, & \\text{si } a \\in best\\_actions, \\\\\n",
    "0, & \\text{si } a \\notin best\\_actions.\n",
    "\\end{cases} \\\\\n",
    "\\\\\n",
    "\\textbf{Retornar:} \\quad & new\\_pi.\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_Q(p, V, gamma=GAMMA, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Calcula el valor Q(s, a) para cada estado y acción.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        V: función de valor, diccionario que mapea estado (x, y) a su valor.\n",
    "        gamma: factor de descuento.\n",
    "    \n",
    "    Retorna:\n",
    "        Q: valor Q(s, a).\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            if not is_done(s):\n",
    "                for a in range(4):\n",
    "                    Q[(s,a)] = sum(prob * (r + gamma * V[s_next]) \n",
    "                                for (prob, s_next, r) in p[(s,a)])\n",
    "            else:\n",
    "                for a in range(4):\n",
    "                    Q[(s,a)] = 0\n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00   -21.00   -23.00   -23.00\n",
      "  -19.00   -21.00   -21.00   -21.00\n",
      "  -21.00   -19.00   -15.00   -15.00\n",
      "  -21.00   -15.00    -1.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00   -15.00   -21.00   -23.00\n",
      "   -1.00   -15.00   -21.00   -23.00\n",
      "  -15.00   -19.00   -21.00   -21.00\n",
      "  -21.00   -21.00   -19.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -1.00   -15.00   -21.00\n",
      "  -15.00   -15.00   -19.00   -21.00\n",
      "  -21.00   -21.00   -21.00   -19.00\n",
      "  -23.00   -23.00   -21.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00   -19.00   -21.00   -21.00\n",
      "  -21.00   -21.00   -19.00   -15.00\n",
      "  -23.00   -21.00   -15.00    -1.00\n",
      "  -23.00   -21.00   -15.00     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_pi_random = calculate_Q(p, V_pi_random)\n",
    "print_state_action_values(Q_pi_random, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00    -5.00    -4.00    -4.00\n",
      "   -5.00    -4.00    -3.00    -3.00\n",
      "   -4.00    -3.00    -2.00    -2.00\n",
      "   -3.00    -2.00    -1.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00    -6.00    -5.00    -4.00\n",
      "   -1.00    -6.00    -5.00    -4.00\n",
      "   -6.00    -5.00    -4.00    -3.00\n",
      "   -5.00    -4.00    -3.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -1.00    -6.00    -5.00\n",
      "   -6.00    -6.00    -5.00    -4.00\n",
      "   -5.00    -5.00    -4.00    -3.00\n",
      "   -4.00    -4.00    -3.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00    -5.00    -4.00    -3.00\n",
      "   -5.00    -4.00    -3.00    -2.00\n",
      "   -4.00    -3.00    -2.00    -1.00\n",
      "   -4.00    -3.00    -2.00     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_da_pi = calculate_Q(p, V_da_pi)\n",
    "print_state_action_values(Q_da_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00  -100.00    -3.94    -3.94\n",
      " -100.00  -100.00    -2.97    -2.97\n",
      " -100.00  -100.00    -1.99    -1.99\n",
      " -100.00  -100.00    -1.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00  -100.00  -100.00    -3.94\n",
      "   -1.00  -100.00  -100.00    -3.94\n",
      " -100.00  -100.00  -100.00    -2.97\n",
      " -100.00  -100.00  -100.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -1.00  -100.00  -100.00\n",
      " -100.00  -100.00  -100.00  -100.00\n",
      " -100.00  -100.00  -100.00  -100.00\n",
      " -100.00  -100.00  -100.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00  -100.00  -100.00    -2.97\n",
      " -100.00  -100.00  -100.00    -1.99\n",
      " -100.00  -100.00  -100.00    -1.00\n",
      " -100.00  -100.00  -100.00     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_d_pi = calculate_Q(p, V_d_pi, gamma=0.99)\n",
    "print_state_action_values(Q_d_pi, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_policy(p, V, gamma=GAMMA, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Genera una política mejorada (greedy) a partir de la función de valor V.\n",
    "    \n",
    "    Parámetros:\n",
    "        env: entorno que posee un atributo 'p' con la dinámica.\n",
    "        V: diccionario que mapea estados a sus valores.\n",
    "        gamma: factor de descuento.\n",
    "    \n",
    "    Retorna:\n",
    "        new_pi: diccionario que representa la política mejorada, mapeando (estado, acción) a probabilidad.\n",
    "    \"\"\"\n",
    "    new_pi = {}\n",
    "    Q = calculate_Q(p, V, gamma)\n",
    "    tol = 1e-4\n",
    "\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            if is_done(s):\n",
    "                continue\n",
    "\n",
    "            mejores_a = [0]\n",
    "            mejor_q = Q[(s,0)]\n",
    "            for a in range(1,4): \n",
    "                q_dif = Q[(s,a)] - mejor_q\n",
    "                if q_dif > tol:\n",
    "                    mejor_q = Q[(s,a)]\n",
    "                    mejores_a = [a]\n",
    "                elif (tol > q_dif or tol == q_dif) and (q_dif > (-tol) or q_dif == (-tol)):\n",
    "                    mejores_a.append(a)\n",
    "\n",
    "            for a in range(4):\n",
    "                new_pi[(s,a)] = 1 / len(mejores_a) if a in mejores_a else 0\n",
    "\n",
    "    return new_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00   -14.00   -20.00   -22.00\n",
      "  -14.00   -18.00   -20.00   -20.00\n",
      "  -20.00   -20.00   -18.00   -14.00\n",
      "  -22.00   -20.00   -14.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_pi_random = policy_evaluation(p, pi_rand)\n",
    "print_state_values(V_pi_random, GRID_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -1.00    -2.00    -3.00\n",
      "   -1.00    -2.00    -3.00    -2.00\n",
      "   -2.00    -3.00    -2.00    -1.00\n",
      "   -3.00    -2.00    -1.00     0.00\n"
     ]
    }
   ],
   "source": [
    "pi_mejorada = improve_policy(p, V_pi_random)\n",
    "V_mejorada = policy_evaluation(p, pi_mejorada)\n",
    "print_state_values(V_mejorada, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.50   0.50   0.00\n",
      "  0.50   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.50   0.00   0.00\n",
      "  1.00   0.50   0.00   0.00\n",
      "  0.50   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.50\n",
      "  0.00   0.50   0.50   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   0.50\n",
      "  0.00   0.00   0.50   1.00\n",
      "  0.00   0.00   0.50   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_policy_grids(pi_mejorada, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_policies(pi_1, pi_2, tol, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Compara dos políticas y retorna True si son iguales (considerando una tolerancia en los valores), o False en caso contrario.\n",
    "    \n",
    "    Parámetros:\n",
    "        pi_1, pi_2: diccionarios que representan las políticas (mapean (estado, acción) a probabilidad).\n",
    "        tol: tolerancia para comparar las probabilidades.\n",
    "    \n",
    "    Retorna:\n",
    "        True si ambas políticas son iguales en todos los (estado, acción); False de lo contrario.\n",
    "    \"\"\"\n",
    "    delta = 0\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            for a in range(4):\n",
    "                if is_done(s):\n",
    "                    continue\n",
    "                distr_1 = pi_1.get(((s), a), 0)\n",
    "                distr_2 = pi_2.get(((s), a), 0)\n",
    "                delta = max(delta, abs(distr_1 - distr_2))\n",
    "    return delta < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(p, pi_init, gamma, theta, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Realiza el algoritmo de iteración de política.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        pi: política óptima.\n",
    "        iter: cantidad de iteraciones necesarias para converger\n",
    "\n",
    "    pi_int la mejoramos\n",
    "    la evaluamos\n",
    "    si no mejora\n",
    "    iteramos\n",
    "    \"\"\"\n",
    "    iter = 0\n",
    "    while True:\n",
    "        iter += 1\n",
    "        V = policy_evaluation(p, pi_init)\n",
    "        pi = improve_policy(p, V)\n",
    "        if compare_policies(pi_init, pi, tol=theta):\n",
    "            break\n",
    "        else:\n",
    "            pi_init = pi\n",
    "    return pi, iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.25   0.00\n",
      "  0.00   0.25   0.50   0.00\n",
      "  0.50   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.50   0.25   0.00\n",
      "  1.00   0.25   0.00   0.00\n",
      "  0.50   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.50\n",
      "  0.00   0.50   0.25   0.00\n",
      "  0.00   0.25   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   0.50\n",
      "  0.00   0.00   0.25   1.00\n",
      "  0.00   0.25   0.50   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star, iter = policy_iteration(p, pi_rand, gamma=GAMMA, theta=1e-6)\n",
    "print(iter)\n",
    "print_policy_grids(pi_star, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1) Explicar por qué esta política óptima tiene dicha distribución de probabilidad. \n",
    "> 2) Existen otras políticas óptimas posibles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Respuesta:\n",
    "\n",
    "1: \n",
    "La distribución de la política se puede explicar de manera intuitiva. Para los estados adyacentes a las casillas terminales, como por ejemplo (0,1) utilizando la notación (fila, col), la distribución de la política es (0, 0, 1, 0), porque la casilla terminal (0,0) está a un movimiento de distancia.\n",
    "\n",
    "Por lo tanto, en este ejemplo, movernos a la izquierda maximiza nuestra recompensa. Lo mismo sucede para todas las casillas que comparten o una fila o una columna con las casillas terminales; el movimiento más corto implica repetir la acción con dirección a la casilla terminal con la que comparte fila o columna.\n",
    "\n",
    "Sin embargo, hay casos interesantes como las casillas (0,3) y (3,0). La casilla (0,3) comparte la fila con una terminal y la columna con otra, y viceversa para (3,0). En estas situaciones, moverse a lo largo de la fila compartida hacia su terminal o moverse a lo largo de la columna compartida hacia la otra terminal son ambas opciones igualmente óptimas en términos de distancia. Por esta razón, la distribución de la política para la casilla (1,1) es (0, 0.5, 0.5, 0), indicando una probabilidad de 0.5 para cada una de las dos acciones óptimas.\n",
    "\n",
    "Por último, en las casillas con fila entre {1, 2} y columna entre {1, 2}, que no comparten ninguna fila o columna con las casillas terminales. Se da que hay dos o incluso cuatro caminos óptimos posibles. Por eso vemos una distribución de 0.5 y 0.25 para las acciones que resultan en esos caminos óptimos.\n",
    "\n",
    "2:\n",
    "Como ya indiqué, pueden existir diversas rutas óptimas. \n",
    "\n",
    "Una política podría limitarse a solo uno de los caminos. Pero como son igualmente óptimos, generaría la misma recompensa que otra política que explore ambos con una distribución equitativa (0.5 cada uno). \n",
    "\n",
    "Así como también sería óptima una política que toma un camino óptimo el 0.3 y 0.7 el otro. Fundamentalmente, múltiples políticas pueden optimizar la recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.00    -1.00    -2.00    -3.00\n",
      "   -1.00    -2.00    -3.00    -2.00\n",
      "   -2.00    -3.00    -2.00    -1.00\n",
      "   -3.00    -2.00    -1.00     0.00\n"
     ]
    }
   ],
   "source": [
    "V_pi_star = policy_evaluation(p, pi_star)\n",
    "print_state_values(V_pi_star, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores para la acción Derecha (acción 0):\n",
      "    0.00    -3.00    -4.00    -4.00\n",
      "   -3.00    -4.00    -3.00    -3.00\n",
      "   -4.00    -3.00    -2.00    -2.00\n",
      "   -3.00    -2.00    -1.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Arriba (acción 1):\n",
      "    0.00    -2.00    -3.00    -4.00\n",
      "   -1.00    -2.00    -3.00    -4.00\n",
      "   -2.00    -3.00    -4.00    -3.00\n",
      "   -3.00    -4.00    -3.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Izquierda (acción 2):\n",
      "    0.00    -1.00    -2.00    -3.00\n",
      "   -2.00    -2.00    -3.00    -4.00\n",
      "   -3.00    -3.00    -4.00    -3.00\n",
      "   -4.00    -4.00    -3.00     0.00\n",
      "\n",
      "\n",
      "Valores para la acción Abajo (acción 3):\n",
      "    0.00    -3.00    -4.00    -3.00\n",
      "   -3.00    -4.00    -3.00    -2.00\n",
      "   -4.00    -3.00    -2.00    -1.00\n",
      "   -4.00    -3.00    -2.00     0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_pi_star = calculate_Q(p, V_pi_star)\n",
    "print_state_action_values(Q_pi_star, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        state = (obs[\"pos\"][0], obs[\"pos\"][1])\n",
    "        action = select_action(state, pi_star)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas con distintas políticas iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba con política determinística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.25   0.00\n",
      "  0.00   0.25   0.50   0.00\n",
      "  0.50   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.50   0.25   0.00\n",
      "  1.00   0.25   0.00   0.00\n",
      "  0.50   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.50\n",
      "  0.00   0.50   0.25   0.00\n",
      "  0.00   0.25   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   0.50\n",
      "  0.00   0.00   0.25   1.00\n",
      "  0.00   0.25   0.50   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star_2, iter_2 = policy_iteration(p, pi_der_aba, gamma=1, theta=1e-6)\n",
    "print(iter_2)\n",
    "print_policy_grids(pi_star_2, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba con política estocástica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{((0, 1), 0): 0.8, ((0, 1), 1): 0.1, ((0, 1), 2): 0.1, ((0, 1), 3): 0, ((0, 2), 0): 0.8, ((0, 2), 1): 0.1, ((0, 2), 2): 0.1, ((0, 2), 3): 0, ((0, 3), 0): 0.8, ((0, 3), 1): 0.1, ((0, 3), 2): 0.1, ((0, 3), 3): 0, ((1, 0), 0): 0.8, ((1, 0), 1): 0.1, ((1, 0), 2): 0.1, ((1, 0), 3): 0, ((1, 1), 0): 0.8, ((1, 1), 1): 0.1, ((1, 1), 2): 0.1, ((1, 1), 3): 0, ((1, 2), 0): 0.8, ((1, 2), 1): 0.1, ((1, 2), 2): 0.1, ((1, 2), 3): 0, ((1, 3), 0): 0.8, ((1, 3), 1): 0.1, ((1, 3), 2): 0.1, ((1, 3), 3): 0, ((2, 0), 0): 0.8, ((2, 0), 1): 0.1, ((2, 0), 2): 0.1, ((2, 0), 3): 0, ((2, 1), 0): 0.8, ((2, 1), 1): 0.1, ((2, 1), 2): 0.1, ((2, 1), 3): 0, ((2, 2), 0): 0.8, ((2, 2), 1): 0.1, ((2, 2), 2): 0.1, ((2, 2), 3): 0, ((2, 3), 0): 0.8, ((2, 3), 1): 0.1, ((2, 3), 2): 0.1, ((2, 3), 3): 0, ((3, 0), 0): 0.8, ((3, 0), 1): 0.1, ((3, 0), 2): 0.1, ((3, 0), 3): 0, ((3, 1), 0): 0.8, ((3, 1), 1): 0.1, ((3, 1), 2): 0.1, ((3, 1), 3): 0, ((3, 2), 0): 0.8, ((3, 2), 1): 0.1, ((3, 2), 2): 0.1, ((3, 2), 3): 0}\n"
     ]
    }
   ],
   "source": [
    "pi_arr_der_izq = {}\n",
    "for x in range(GRID_SIZE):\n",
    "    for y in range(GRID_SIZE):\n",
    "        if is_done((x, y)): # Si no estamos en un estado terminal\n",
    "                continue\n",
    "        pi_arr_der_izq[((x, y), 0)] = 0.8\n",
    "        pi_arr_der_izq[((x, y), 1)] = 0.1\n",
    "        pi_arr_der_izq[((x, y), 2)] = 0.1\n",
    "        pi_arr_der_izq[((x, y), 3)] = 0\n",
    "print(pi_arr_der_izq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.25   0.00\n",
      "  0.00   0.25   0.50   0.00\n",
      "  0.50   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.50   0.25   0.00\n",
      "  1.00   0.25   0.00   0.00\n",
      "  0.50   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.50\n",
      "  0.00   0.50   0.25   0.00\n",
      "  0.00   0.25   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   0.50\n",
      "  0.00   0.00   0.25   1.00\n",
      "  0.00   0.25   0.50   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star_3, iter_3 = policy_iteration(p, pi_arr_der_izq, gamma=1, theta=1e-6)\n",
    "print(iter_3)\n",
    "print_policy_grids(pi_star_3, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusión\n",
    "\n",
    "Se observa que toda política inicial escogida termina derivando la misma política óptima. \n",
    "\n",
    "La política aleatoria converge más rápido, necesitando solo 3 iteraciones frente a las 4 de las otras. \n",
    "\n",
    "Esto sugiere que su inicio con igual probabilidad para cada acción se parece más a la política óptima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration es un algoritmo de programación dinámica empleado para estimar la función de valor óptima $ V^* $ y, a partir de ella, derivar la política óptima $ \\pi^* $. Este método se basa en actualizar iterativamente los valores de cada estado utilizando la ecuación de Bellman, hasta que las actualizaciones sean menores que un pequeño umbral $ \\theta $ que determina la precisión de la estimación.\n",
    "\n",
    "El proceso general es el siguiente:\n",
    "\n",
    "1. **Inicialización:**  \n",
    "   Se asigna un valor arbitrario a $ V(s) $ para todos los estados $ s \\in S^+ $, excepto en los estados terminales, donde se establece $ V(\\text{terminal}) = 0 $.\n",
    "\n",
    "2. **Actualización Iterativa:**  \n",
    "   Para cada estado $ s \\in S $, se actualiza su valor mediante la fórmula:\n",
    "   $$\n",
    "   V(s) \\leftarrow \\max_{a} \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma V(s') \\Bigr],\n",
    "   $$\n",
    "   donde $ p(s', r \\mid s, a) $ es la probabilidad de transitar al estado $ s' $ y recibir la recompensa $ r $ al tomar la acción $ a $ en el estado $ s $.  \n",
    "   La iteración continúa hasta que la diferencia máxima entre los valores antiguos y actualizados en todos los estados sea menor que $ \\theta $.\n",
    "\n",
    "3. **Extracción de la Política:**  \n",
    "   Una vez convergido $ V $, se define la política óptima determinista $ \\pi^* $ de la siguiente manera:\n",
    "   $$\n",
    "   \\pi^*(s) = \\operatorname*{argmax}_{a} \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma V(s') \\Bigr].\n",
    "   $$\n",
    "\n",
    "Este algoritmo es fundamental en el aprendizaje por refuerzo y en la resolución de procesos de decisión de Markov, ya que permite obtener de manera eficiente tanto la función de valor óptima como la política que maximiza el retorno esperado en cada estado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dif_dict(dict_1, dict_2):\n",
    "    ''''\n",
    "    Para dos diccionarios con claves idénticas y no vacíos. Devuelve la mayor diferencia uno a uno entre los dos diccionarios.\n",
    "    Parámetros:\n",
    "        dict_1: primer diccionario\n",
    "        dict_2: segundo diccionario\n",
    "\n",
    "    Retorna:\n",
    "        max_dif: máxima diferencia entre los mismos\n",
    "    '''\n",
    "    max_dif = 0\n",
    "    for x in dict_1: \n",
    "        val_1 = dict_1[x]\n",
    "        val_2 = dict_2[x]\n",
    "        dif = val_1 - val_2\n",
    "        if dif > max_dif:\n",
    "            max_dif = dif\n",
    "\n",
    "    return max_dif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(p, gamma=GAMMA, theta=1e-5, size=GRID_SIZE):\n",
    "    \"\"\"\n",
    "    Realiza el algoritmo de iteración de valor.\n",
    "    \n",
    "    Parámetros:\n",
    "        p: dinámica del entorno, diccionario que mapea (estado, acción) a una lista de (probabilidad, estado, recompensa).\n",
    "        gamma: factor de descuento.\n",
    "        theta: umbral de convergencia.\n",
    "    \n",
    "    Retorna:\n",
    "        pi: la pólitica óptima.\n",
    "    \"\"\"\n",
    "    V_init = {}\n",
    "    # Inicializo V con valores arbitrarios\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x,y)\n",
    "            if is_done(s):\n",
    "                V_init[s] = 0\n",
    "            else:\n",
    "                V_init[s] = -6\n",
    "\n",
    "    V = V_init.copy()\n",
    "    # Iteración para actualizar el valor de V\n",
    "    while True:\n",
    "        for y in range(4):\n",
    "            for x in range(4):\n",
    "                s = (x,y)\n",
    "                if is_done(s):\n",
    "                    continue\n",
    "                mejor_v = -np.inf\n",
    "                for a in range(4):\n",
    "                    new_v = sum(prob * (r + gamma * V[s_next]) \n",
    "                                for (prob, s_next, r) in p[(s,a)])\n",
    "                    if new_v > mejor_v:\n",
    "                        mejor_v = new_v\n",
    "                V[s] = mejor_v\n",
    "        # Evaluó máxima diferencia entre valores actualizados y antiguos\n",
    "        max_dif = max_dif_dict(V, V_init)\n",
    "        if theta > max_dif:\n",
    "            break\n",
    "        else:\n",
    "            V_init = V\n",
    "\n",
    "    pi = {}\n",
    "    # Extraigo la mejor política\n",
    "    for y in range(size):\n",
    "        for x in range(size):\n",
    "            s = (x, y)\n",
    "            if is_done(s):\n",
    "                pi[s] = 0\n",
    "                continue\n",
    "            a_elegida = []\n",
    "            mejor_valor_a = -np.inf\n",
    "            for a in range(4):\n",
    "                valor_a = 0\n",
    "                for prob, s_next, r in p[(s, a)]:\n",
    "                    valor_a += prob * (r + gamma * V[s_next])\n",
    "                if valor_a > mejor_valor_a:\n",
    "                    mejor_valor_a = valor_a\n",
    "                    a_elegida = [a]\n",
    "                elif valor_a == mejor_valor_a:\n",
    "                    a_elegida.append(a)\n",
    "            for a in range(4):\n",
    "                pi[(s,a)] = 1 / len(a_elegida) if a in a_elegida else 0 \n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política para la acción Derecha (acción 0):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  0.00   0.00   0.25   0.00\n",
      "  0.00   0.25   0.50   0.00\n",
      "  0.50   1.00   1.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Arriba (acción 1):\n",
      "  0.00   0.00   0.00   0.00\n",
      "  1.00   0.50   0.25   0.00\n",
      "  1.00   0.25   0.00   0.00\n",
      "  0.50   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Izquierda (acción 2):\n",
      "  0.00   1.00   1.00   0.50\n",
      "  0.00   0.50   0.25   0.00\n",
      "  0.00   0.25   0.00   0.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n",
      "Política para la acción Abajo (acción 3):\n",
      "  0.00   0.00   0.00   0.50\n",
      "  0.00   0.00   0.25   1.00\n",
      "  0.00   0.25   0.50   1.00\n",
      "  0.00   0.00   0.00   0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_star_4 = value_iteration(p)\n",
    "print_policy_grids(pi_star_4, GRID_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_environment()\n",
    "\n",
    "for episode_num in range(10):\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    episode_over = False\n",
    "    while not episode_over:\n",
    "        state = (obs[\"pos\"][0], obs[\"pos\"][1])\n",
    "        action = select_action(state, pi_star_4)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tareas**\n",
    "\n",
    "**1. Comparación entre diferentes políticas iniciales**\n",
    "- Prueba iniciar la **Iteración de Política** con diferentes políticas iniciales (aleatorias, deterministas, uniformes).\n",
    "- ¿La política óptima cambia en función de la inicialización?\n",
    "- ¿Cuántas iteraciones necesita cada caso para converger?\n",
    "  \n",
    "**2. [extra] Impacto de la estructura del Grid World**\n",
    "- Modifica el tamaño de la grilla y analiza su impacto en el desempeño de los algoritmos.\n",
    "\n",
    "**3. [extra] Evaluación en un entorno estocástico**\n",
    "- Introduce aleatoriedad en la transición de estados (por ejemplo, que el agente no siempre se mueva exactamente en la dirección deseada). (usar \"gymnasium_env/GridWorld_stochastic-v0\")\n",
    "- ¿Cómo cambia la política óptima en este caso?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia-taller",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
